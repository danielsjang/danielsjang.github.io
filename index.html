<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Dong-Ki Kim</title>
  
  <meta name="author" content="Dong-Ki Kim">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/mit_seal.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Dong-Ki Kim</name>
              </p>
              <p>
                I am a graduate student at <a href="https://lids.mit.edu" target="_blank">MIT-LIDS</a> advised by <a href="https://scholar.google.com/citations?user=gX7rSCcAAAAJ&hl=en" target="_blank">Professor Jonathan P. How</a>.
                My research interests include reinforcement learning, meta-learning, and hierarchical learning.
              </p>
              <p>
              I am grateful to have been advised by such wonderful advisors: 
              <a href="https://www.ri.cmu.edu/ri-faculty/sebastian-scherer/" target="_blank">Professor Sebastian Scherer</a> at <a href="https://www.ri.cmu.edu/" target="_blank">CMU-RI</a>, <a href="http://ttic.uchicago.edu/~mwalter/" target="_blank">Professor Matthew R. Walter</a> at <a href="http://www.ttic.edu/" target="_blank">TTIC</a>, and <a href="http://www.nus.edu.sg/about/management/chen-tsuhan" target="_blank">Professor Tsuhan Chen</a> at <a href="http://www.cornell.edu/" target="_blank">Cornell</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:dkkim93@mit.edu">Email</a> &nbsp/&nbsp
                <a href="resume/Kim_DongKi_Resume.pdf" target="_blank">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=Yl_3akYAAAAJ&hl=en" target="_blank">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/dkkim93" target="_blank"> GitHub </a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img src="images/dongki.jpg" width="120" style="border-radius:50%" class="profile-image"> 
            </td>
          </tr>
        </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">06/2019: Our work on <a href="https://arxiv.org/pdf/1903.06592.pdf" target="_blank">multi-agent knowledge sharing</a> is accepted to <a href="https://www.iros2019.org/" target="_blank">IROS-19</a></li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">01/2019: Our work on <a href="https://arxiv.org/abs/1805.07830" target="_blank">learning to teach</a> is selected for <a href="https://aaai.org/Awards/paper.php" target="_blank">outstanding student paper honorable mention</a> for <a href="https://aaai.org/Conferences/AAAI-19/" target="_blank">AAAI-19</a></li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">11/2018: Our work on <a href="https://arxiv.org/pdf/1903.03216.pdf" target="_blank">hierarchical teaching</a> is accepted to <a href="http://aaai-rlg.mlanctot.info/index.html" target="_blank">AAAI-19 workshop</a>.</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                Below is a selection of my research projects related to reinforcement learning and robot perception.
              </p>
            </td>
          </tr>
        </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/distillation.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Policy Distillation and Value Matching in Multiagent Reinforcement Learning</papertitle>
              <br>
              <a href="http://www.samirw.com/", target="_blank">Samir Wadhwania</a>, 
              <strong>Dong-Ki Kim</strong>, 
              <a href="http://www.mit.edu/~shayegan/" target="_blank">Shayegan Omidshafiei</a>, 
              <a href="https://scholar.google.com/citations?user=gX7rSCcAAAAJ&hl=en", target="_blank">Jonathan P. How</a>
              <br>
              IROS-19
              <br>
              <a href="https://arxiv.org/pdf/1903.06592.pdf">Paper</a>
              <p></p>
              <p>
                We introduce a multi-agent algorithm that combines knowledge from agents through distillation and value-matching (DVM). DVM outperforms policy distillation alone and allows faster learning in dynamic tasks.
              </p>
            </td>
          </tr>
          <br/>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/hierarchical_teaching.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Learning Hierarchical Teaching in Cooperative Multiagent Reinforcement Learning</papertitle>
              <br>
              <strong>Dong-Ki Kim</strong>, 
              <a href="http://www.mit.edu/~miaoliu/", target="_blank">Miao Liu</a>, 
              <a href="http://www.mit.edu/~shayegan/" target="_blank">Shayegan Omidshafiei</a>, 
              <a href="http://acl.mit.edu/people/slcot" target="_blank">Sebastian Lopez-Cot</a>, 
              <a href="https://scholar.google.com/citations?user=PK7UzAwAAAAJ&hl=en" target="_blank">Matthew Riemer</a>, 
              <a href="https://scholar.google.com/citations?user=hU-LeNEAAAAJ&hl=en" target="_blank">Golnaz Habibi</a>, 
              <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-gtesauro" target="_blank">Gerald Tesauro</a>, 
              <a href="https://www.linkedin.com/in/samimourad/" target="_blank">Sami Mourad</a>, 
              <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-mcam" target="_blank">Murray Campbell</a>, 
              <a href="https://scholar.google.com/citations?user=gX7rSCcAAAAJ&hl=en", target="_blank">Jonathan P. How</a>
              <br>
              AAAI-19 Workshop
              <br>
              <a href="https://arxiv.org/pdf/1903.03216.pdf" target="_blank">Paper</a> /
              <a href="https://www.wired.com/brandlab/2019/06/robotic-future-bots-operate-together-learn/" target="_blank">WIRED News</a>
              <p></p>
              <p>
                We introduce a new learning to teach framework, called Hierarchical MultiagentTeaching (HMAT).
                Our framework solves difficulties faced by previous learning to teach works when operating in domains with long horizons, large state spaces, and continuous actions.
              </p>
            </td>
          </tr>
          <br/>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/lectr.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Learning to Teach in Cooperative Multiagent Reinforcement Learning</papertitle>
              <br>
              <a href="http://www.mit.edu/~shayegan/" target="_blank">Shayegan Omidshafiei</a>, 
              <strong>Dong-Ki Kim</strong>, 
              <a href="http://www.mit.edu/~miaoliu/", target="_blank">Miao Liu</a>, 
              <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-gtesauro" target="_blank">Gerald Tesauro</a>, 
              <a href="https://scholar.google.com/citations?user=PK7UzAwAAAAJ&hl=en" target="_blank">Matthew Riemer</a>, 
              <a href="https://www.khoury.northeastern.edu/people/chris-amato/" target="_blank">Christopher Amato</a>, 
              <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-mcam" target="_blank">Murray Campbell</a>, 
              <a href="https://scholar.google.com/citations?user=gX7rSCcAAAAJ&hl=en", target="_blank">Jonathan P. How</a>
              <br>
              AAAI-19 <font color="red"><strong>(Outstanding Student Paper Honorable Mention)</strong></font>, <a href="https://sites.google.com/view/llarla2018/home" target="_blank">ICML18 Workshop</a>
              <br>
              <a href="https://arxiv.org/pdf/1903.03216.pdf" target="_blank">Paper</a> /
              <a href="http://news.mit.edu/2019/learning-to-teach-to-speed-up-learning-0129" target="_blank">MIT News</a>
              <p></p>
              <p>
                This paper presents Learning to Coordinate and Teach Reinforcement (LeCTR), the first general framework for intelligent agents to learn to teach in a cooperative MARL. 
              </p>
            </td>
          </tr>
        </tbody>
        </table>

      </td>
    </tr>
  </table>
</body>

</html>
