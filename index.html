<!DOCTYPE HTML>
<!--
    Twenty by HTML5 UP
    html5up.net | @ajlkn
    Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
    <head>
        <title>Dong-Ki's Website</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
        <link href="assets/css/bootstrap.min.css" rel="stylesheet">
        <link rel="stylesheet" href="assets/css/main.css" />
        <noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
    </head>
    <body class="index is-preload">
        <div id="page-wrapper">
            <!-- Header -->
            <header id="header" class="alt">
                <h1 id="logo"><a href="index.html">Dong-Ki<span>'s Website</span></a></h1>
            </header>

            <!-- Banner -->
            <section id="banner">
                <!--
                    ".inner" is set up as an inline-block so it automatically expands
                    in both directions to fit whatever's inside it. This means it won't
                    automatically wrap lines, so be sure to use line breaks where
                    appropriate (<br />).
                -->
                <div class="inner">
                    <header>
                        <h2>Dong-Ki Kim</h2>
                    </header>

                    <div class="col-me-12 h-100">
                        <div class="row align-items-center h-100">
                            <div class="col-md-4">
                                <img src="images/dongki.jpg" width="120" style="border-radius:50%" class="profile-image"> 
                            </div>
                            <div class="col-md-8 text-sm-center text-md-left">
                                <i class="fa fa-envelope fa-fw"></i> &nbsp; dkkim93@mit.edu<br>
                                <i class="fa fa-google fa-fw"></i> &nbsp; <a href="https://scholar.google.com/citations?user=Yl_3akYAAAAJ&hl=en" target="_blank" style="color:inherit;text-decoration:none">Google Scholar</a><br>
                                <i class="fa fa-github fa-fw"></i> &nbsp; <a href="https://github.com/dkkim93" target="_blank" style="color:inherit;text-decoration:none">GitHub</a><br>
                            </div> 
                        </divi>
                    </div>

                    <footer>
                        <ul class="buttons stacked">
                            <li><a href="resume/Kim_DongKi_Resume.pdf" target="_blank" class="button fit scrolly">RESUME</a></li>
                        </ul>
                    </footer>
                </div>
            </section>

            <!-- Main -->
            <article id="main">
                <header class="special container">
                    <span class="icon fa-user-o"></span>
                    <h2>
                        <strong>ABOUT ME</strong>
                    </h2>
                    <p>
                    I am a graduate student at <a href="http://mit.edu/" target="_blank">MIT</a>-<a href="https://lids.mit.edu" target="_blank">LIDS</a>, 
                    advised by <a href="http://www.mit.edu/~jhow/" target="_blank">Professor Jonathan P. How</a>.</br>
                    My research interests include:</br>
                    <strong>Reinforcement Learning (RL): </strong>Meta-Learning, Hierarchical Learning, Multiagent RL</br>
                    <strong>Robot Perception: </strong> Semantic Segmentation, Sensor Fusion, Mapping</br></br>

                    I am grateful to have been advised by such wonderful advisors:
                    </br>
                </header>

                <section class="wrapper style1 container special">
                    <div class="row">
                        <div class="col-3 col-12-narrower exp">
                            <section>
                                <img src="images/mit_seal.png" height="130"> 
                                <header>
                                    <h3><strong><a href="https://lids.mit.edu/" target="_blank">LIDS</a> @ <a href="http://web.mit.edu/" target="_blank">MIT</a></strong></h3>
                                    <h4><a href="http://www.mit.edu/~jhow/" target="_blank"><strong>Prof. Jonathan P. How</strong></a></h4>
                                    <h4 class="special-h4"><strong>Grad. Student (Sep17 - Now)</strong></h4>
                                </header>
                                <ul>
                                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><strong>AAAI19:</strong> Learning to teach in cooperative multiagent RL</li>
                                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><strong>AAMAS18:</strong> Attention for multimodal hierarchical RL</li>
                                </ul>
                            </section>
                        </div>

                        <div class="col-3 col-12-narrower exp">
                            <section>
                                <img src="images/cmu_seal.png" height="130"> 
                                <header>
                                    <h3><strong><a href="http://theairlab.org/" target="_blank">The Air Lab</a> @ <a href="https://www.ri.cmu.edu/" target="_blank">CMU-RI</a></strong></h3>
                                    <h4><strong><a href="https://www.ri.cmu.edu/ri-faculty/sebastian-scherer/" target="_blank">Prof. Sebastian Scherer</a></strong></h4>
                                    <h4 class="special-h4"><strong>Intern (Aug16 - Jul17)</strong></h4>
                                </header>
                                <ul>
                                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Perception for an off-road autonomous vehicle</li>
                                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><strong>FSR18:</strong> Multimodal network for semantic segmentation</li>
                                </ul>
                            </section>
                        </div>

                        <div class="col-3 col-12-narrower exp">
                            <section>
                                <img src="images/ttic_seal_v2.png" height="130"> 
                                <header>
                                    <h3><strong><a href="http://www.ttic.edu/ripl/" target="_blank">RIPL</a> @ <a href="http://www.ttic.edu/" target="_blank">TTIC</a></strong></h3>
                                    <h4><strong><a href="http://ttic.uchicago.edu/~mwalter/" target="_blank">Prof. Matthew R. Walter</a></strong></h4>
                                    <h4 class="special-h4"><strong>Intern (Jan16 - Jul16)</strong></h4>
                                </header>
                                <ul>
                                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><strong>ICRA17:</strong> Cross-view visual localization</li>
                                </ul>
                            </section>
                        </div>
                        <div class="col-3 col-12-narrower exp">
                            <section>
                                <img src="images/cornell_seal.png" height="130"> 
                                <header>
                                    <h3><strong><a href="http://chenlab.ece.cornell.edu/" target="_blank">AMP Lab</a> @ <a href="http://www.cornell.edu/" target="_blank">Cornell</a></strong></h3>
                                    <h4><strong><a href="http://www.nus.edu.sg/about/management/chen-tsuhan" target="_blank">Prof. Tsuhan Chen</a></strong></h4>
                                    <h4 class="special-h4"><strong>Undergrad. Student (May14 - Jan16)</strong></h4>
                                </header>
                                <ul>
                                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Visual autonomous navigation</li>
                                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><strong>ICCV15:</strong> Indoor localization by using floor plans</li>
                                </ul>
                            </section>
                        </div>
                    </div>
                </section>

                <section class="wrapper style3 container special">
                    <header class="special container">
                        <span class="icon fa-flask"></span>
                        <h2>
                            <strong>RESEARCH</strong>
                        </h2>
                        <p>
                        Below is a selection of my projects related to <strong>reinforcement learning</strong> and <strong>robot perception</strong>.
                        </p>
                    </header>

                    <header class="major">
                        <h2>Reinforcement Learning</h2>
                    </header>

                    <div class="row">
                        <div class="col-6 col-12-narrower">
                            <section>
                                <img src="images/hierarchical_teaching.png" height="230"> 
                                <header>
                                    <h4 class="research-h4"><strong>Heterogeneous Knowledge Transfer via Hierarchical Teaching in Cooperative Multiagent Reinforcement Learning (<a href="http://aaai-rlg.mlanctot.info/index.html" target="_blank">AAAI19 Workshop</a>)</strong></h4>
                                    <h4 class="author-h4"><strong>Dong-Ki Kim</strong>, Miao Liu, Shayegan Omidshafiei, Sebastian Lopez-Cot, Matthew Riemer, Gerald Tesauro, Murray Campbell, Golnaz Habibi, and Jonathan P. How</h4>
                                </header>
                                <p style="text-align:left;margin:0 0 1em 0;">
                                We introduce a new learning to teach framework, Hierarchical MultiagentTeaching (HMAT).
                                Our framework solves difficulties faced by previous learning to teach works when operating in domains with long horizons, large state spaces, and continuous actions.</br>
                                <center>A preprint version will be available shortly.</center>
                                </p>
                            </section>
                        </div>
                        <div class="col-6 col-12-narrower">
                            <section>
                                <img src="images/lectr.png" height="230"> 
                                <header>
                                    <h4 class="research-h4"><strong>Learning to Teach in Cooperative Multiagent Reinforcement Learning (AAAI19, <a href="https://sites.google.com/view/llarla2018/home" target="_blank">ICML18 Lifelong Learning Workshop</a>)</br></strong></h4>
                                    <h4 class="author-h4">Shayegan Omidshafiei, <strong>Dong-Ki Kim</strong>, Miao Liu, Gerald Tesauro, Matthew Riemer, Christopher Amato, Murray Campbell, and Jonathan P. How</h4>
                                </header>
                                <p style="text-align:left;margin:0 0 1em 0;"></br>
                                This paper presents Learning to Coordinate and Teach Reinforcement (LeCTR), the first general framework for intelligent agents to learn to
                                teach in a cooperative multiagent RL. 
                                </p></br>
                                <footer>
                                    <ul class="buttons">
                                        <li><a href="https://arxiv.org/pdf/1805.07830.pdf" target="_blank" class="button primary">Paper</a></li>
                                    </ul>
                                </footer>
                            </section>

                        </div>
                    </div>

                    <div class="row" style="padding-top:2em;">
                        <div class="col-6 col-12-narrower">
                            <section>
                                <img src="images/casl.png" height="230"> 
                                <header>
                                    <h4 class="research-h4"><strong>Crossmodal Attentive Skill Learner (AAMAS18, NIPS17 Deep RL Symposium)</strong></h4>
                                    <h4 class="author-h4">Shayegan Omidshafiei, <strong>Dong-Ki Kim</strong>, Jason Pazis, and Jonathan P. How</h4>
                                </header>
                                <p style="text-align:left;margin:0 0 1em 0;">
                                This paper introduces the Crossmodal Attentive Skill Learner (CASL), integrated with the recently-introduced Asynchronous Advantage
                                Option-Critic architecture to enable hierarchical RL across multiple sensory inputs.
                                </p>
                                <footer>
                                    <ul class="buttons">
                                        <li><a href="https://arxiv.org/pdf/1711.10314.pdf" target="_blank" class="button primary">Paper</a></li>
                                        <li><a href="https://github.com/shayegano/CASL" target="_blank" class="button primary">Code</a></li>
                                        <li><a href="https://www.youtube.com/watch?v=pj8tva5YayA" target="_blank" class="button primary">Video</a></li>
                                    </ul>
                                </footer>
                            </section>
                        </div>
                    </div>
                </section>

                <section class="wrapper style3 container special">
                    <header class="major">
                        <h2>Robot Perception & Computer Vision</h2>
                    </header>

                    <div class="row">
                        <div class="col-6 col-12-narrower">
                            <section>
                                <img src="images/yamaha.png" height="230"> 
                                <header>
                                    <h4 class="research-h4"><strong>Yamaha Autonomous Off-Road Vehicle Project</strong></h4>
                                    <h4 class="author-h4">CMU-RI and Yamaha Team</h4></br></br>
                                </header>
                                <p style="text-align:left;margin:0 0 1em 0;">We developed an autonomous off-road vehicle (video1).
                                My contributions to the project include a ROS-based system that estimates terrain roughness from 3D point cloud in real-time (video2, video3).
                                </p>
                                <footer>
                                    <ul class="buttons">
                                        <li><a href="https://www.youtube.com/watch?v=mtFchoPOrQo" target="_blank" class="button primary">Video1</a></li>
                                        <li><a href="https://www.youtube.com/watch?v=4FSQ02p4pKI" target="_blank" class="button primary">Video2</a></li>
                                        <li><a href="https://www.youtube.com/watch?v=XCbOBS0n0hE" target="_blank" class="button primary">Video3</a></li>
                                    </ul>
                                </footer>
                            </section>
                        </div>
                        <div class="col-6 col-12-narrower">
                            <section>
                                <img src="images/fsr.png" height="230"> 
                                <header>
                                    <h4 class="research-h4"><strong>Season-Invariant Semantic Segmentation with A Deep Multimodal Network (FSR18)</strong></h4>
                                    <h4 class="author-h4"><strong>Dong-Ki Kim</strong>, Daniel Maturana, Masashi Uenoyama, and Sebastian Scherer</h4>
                                </header>
                                <p style="text-align:left;margin:0 0 1em 0;">
                                We propose a novel multimodal architecture consisting of two streams, image (2D) and LiDAR (3D).
                                By combining the two streams, we achieve a robust season-invariant semantic segmentation.
                                </p>
                                <footer>
                                    <ul class="buttons">
                                        <li><a href="http://www.fsr.ethz.ch/papers/FSR_2017_paper_23.pdf" target="_blank" class="button primary">Paper</a></li>
                                    </ul>
                                </footer>
                            </section>
                        </div>
                    </div>
                    <div class="row" style="padding-top:2em;">
                        <div class="col-6 col-12-narrower">
                            <section>
                                <img src="images/localization.png" height="230"> 
                                <header>
                                    <h4 class="research-h4"><strong>Satellite Image-based Localization via Learned Embeddings (ICRA17)</strong></h4>
                                    <h4 class="author-h4"><strong>Dong-Ki Kim</strong> and Matthew R. Walter</h4>
                                </header>
                                <p style="text-align:left;margin:0 0 1em 0;">
                                We propose a vision-based method that localizes a ground vehicle using publicly available satellite imagery as the
                                only prior knowledge of the environment.</br>
                                </p>
                                <footer>
                                    <ul class="buttons">
                                        <li><a href="https://arxiv.org/pdf/1704.01133.pdf" target="_blank" class="button primary">Paper</a></li>
                                        <li><a href="https://www.youtube.com/watch?v=58K1-0WpGNs" target="_blank" class="button primary">Video</a></li>
                                    </ul>
                                </footer>
                            </section>
                        </div>
                        <div class="col-6 col-12-narrower">
                            <section>
                                <img src="images/iccv.png" height="230"> 
                                <header>
                                    <h4 class="research-h4"><strong>You Are Here: Mimicking the Human Thinking Process in Reading Floor-Plans (ICCV15)</strong></h4>
                                    <h4 class="author-h4">Hang Chu, <strong>Dong-Ki Kim</strong>, and Tsuhan Chen</h4>
                                </header>
                                <p style="text-align:left;margin:0 0 1em 0;">
                                We address the problem of locating a user in a floor-plan, by using a camera and a floor-plan.</br></br>
                                </p>
                                <footer>
                                    <ul class="buttons">
                                        <li><a href="http://chenlab.ece.cornell.edu/people/Hang/publications/Hang_ICCV15.pdf" target="_blank" class="button primary">Paper</a></li>
                                        <li><a href="https://vimeo.com/142409054" target="_blank" class="button primary">Video</a></li>
                                    </ul>
                                </footer>
                            </section>
                        </div>
                    </div>
                    <div class="row" style="padding-top:2em;">
                        <div class="col-6 col-12-narrower">
                            <section>
                                <img src="images/drone_nav.png" height="230"> 
                                <header>
                                    <h4 class="research-h4"><strong>Deep Neural Network for Real-Time Autonomous Indoor Navigation (Technical Report)</strong></h4>
                                    <h4 class="author-h4"><strong>Dong-Ki Kim</strong> and Tsuhan Chen</h4>
                                </header>
                                <p style="text-align:left;margin:0 0 1em 0;">
                                In this paper, we propose a vision-based system in which a drone autonomously navigates indoors and
                                finds a specific target.
                                </p>
                                <footer>
                                    <ul class="buttons">
                                        <li><a href="https://arxiv.org/pdf/1511.04668.pdf" target="_blank" class="button primary">Paper</a></li>
                                        <li><a href="https://www.youtube.com/watch?v=2Y08GRYnC3U" target="_blank" class="button primary">Video</a></li>
                                    </ul>
                                </footer>
                            </section>
                        </div>
                    </div>
                </section>

                </article>

            <!-- Footer -->
            <footer id="footer">
                <ul class="copyright">
                    <li>&copy; Dong-Ki</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
                </ul>
            </footer>
        </div>

        <!-- Scripts -->
            <script src="assets/js/jquery.min.js"></script>
            <script src="assets/js/jquery.dropotron.min.js"></script>
            <script src="assets/js/jquery.scrolly.min.js"></script>
            <script src="assets/js/jquery.scrollex.min.js"></script>
            <script src="assets/js/browser.min.js"></script>
            <script src="assets/js/breakpoints.min.js"></script>
            <script src="assets/js/util.js"></script>
            <script src="assets/js/main.js"></script>
    </body>
</html>
